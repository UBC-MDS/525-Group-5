{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "\n",
    "In this milestone, we are using the Figshare API to pull data and analyze it in upcoming milestones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data from Figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily rainfall over NSW, Australia\n",
    "# https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681\n",
    "article_id = 14096681\n",
    "# Metadata for the download\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the associated dataset\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve `data.zip`\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for f in files:\n",
    "    if f[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(f[\"download_url\"], f\"{output_directory}/{f['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract `data.zip`\n",
    "output_zip_file = os.path.join(output_directory, \"./data.zip\")\n",
    "with zipfile.ZipFile(output_zip_file, 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather a list of files of CSV to merge\n",
    "files = glob.glob(f'{output_directory}/*.csv')\n",
    "files.remove(f'{output_directory}/observed_daily_rainfall_SYD.csv')\n",
    "# files = files[0:1]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_merge = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "\n",
    "combined_path = f'{output_directory}/combined_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# files = glob.glob('dailyrainfall/*.csv')\n",
    "# df = pd.concat((pd.read_csv(files, index_col=0)\n",
    "#                 .assign(model=re.findall(\"/([^_]*)\", file)[0])\n",
    "#                 for file in files)\n",
    "#               )\n",
    "\n",
    "# A Pythonic way (but not the most memory-efficient way) for merging the data\n",
    "df = pd.concat((pd.read_csv(f, index_col=0, usecols=columns_to_merge)\n",
    "                .assign(model=f[len(output_directory)+1:-len(\"_daily_rainfall_NSW.csv\")])\n",
    "                for f in files)\n",
    "              )\n",
    "df.to_csv(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken  |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-----------:|\n",
    "> | Chen, Ziyi   | OSX 13.2.1       | 32GB | M1 (10 processors)| YES    |  3min 30s   |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    |  5min 56s   |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    |  3min 17s   |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    |  10min 6s   |\n",
    "> \n",
    "> Table 1: Time taken to combine the CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This is the baseline time needed to load the CSV file as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(f\"{output_directory}/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | OSX 13.2.1       | 32GB | M1 (10 processors)| YES    | 34s           | 3.3+ GB      |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 1min 16s      | 3.3+ GB      |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 31.6s         | 3.3+ GB      |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 2min 45s      | 3.3+ GB      |\n",
    "> \n",
    "> Table 2: Time taken to read the combined CSV (baseline)\n",
    "\n",
    "#### Observations from Baseline\n",
    "\n",
    "(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Change the `dtype` of the data\n",
    "\n",
    "We notice that by default it uses `float64` if we do not specify it. First, we try to see if switching to `float32` would make a smaller memory footprint, as well as a faster time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_float32 = pd.read_csv(f\"{output_directory}/combined_data.csv\", dtype={\n",
    "    'lat_min': 'float32',\n",
    "    'lat_max': 'float32',\n",
    "    'lon_min': 'float32',\n",
    "    'lon_max': 'float32',\n",
    "    'rain (mm/day)': 'float32'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float32.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | OSX 13.2.1       | 32GB | M1 (10 processors)| YES    | 31.7s         | 2.1+ GB      |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 1min 11s      | 2.1+ GB      |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 30.5s         | 2.1+ GB      |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 2min 28s      | 2.1+ GB      |\n",
    "> \n",
    "> Table 3: Time taken to read the combined CSV (approach 1: use `float32` instead of `float64`)\n",
    "\n",
    "#### Observations from Approach 1\n",
    "\n",
    "(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Load only column(s) we want\n",
    "\n",
    "The dataset contains a number of columns that we may not need to use in one go. In this approach, we try to just load one column from the combined CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_only_rain = pd.read_csv(f\"{output_directory}/combined_data.csv\", usecols=[\"rain (mm/day)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_only_rain.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | OSX 13.2.1       | 32GB | M1 (10 processors)| YES    | 16.1s         | 476.6 MB     |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 37.1s         | 476.6 MB     |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 14.9s         | 476.6 MB     |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 1min 6s       | 476.6 MB     |\n",
    "> \n",
    "> Table 4: Time taken to read the combined CSV (approach 2: just load `rain (mm/day)`)\n",
    "\n",
    "#### Observations from Approach 2\n",
    "\n",
    "(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explore the EDA in R instead of Python. We try \"exporting\" our data frame as a Parquet file for processing in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(f\"{output_directory}/combined_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why we choose Parquet?\n",
    "> \n",
    "> (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "r_parquet <- open_dataset(\"../data/combined_data.parquet\")\n",
    "r_df <- r_parquet |> collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525_2023]",
   "language": "python",
   "name": "conda-env-525_2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
