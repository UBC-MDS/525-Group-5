{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "\n",
    "In this milestone, we are using the Figshare API to pull data and analyze it in upcoming milestones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data from Figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily rainfall over NSW, Australia\n",
    "# https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681\n",
    "article_id = 14096681\n",
    "# Metadata for the download\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 26579150,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'size': 58863,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e'},\n",
       " {'id': 26579171,\n",
       "  'name': 'environment.yml',\n",
       "  'size': 192,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34'},\n",
       " {'id': 26586554,\n",
       "  'name': 'README.md',\n",
       "  'size': 5422,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c'},\n",
       " {'id': 26766812,\n",
       "  'name': 'data.zip',\n",
       "  'size': 814041183,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9'},\n",
       " {'id': 26766815,\n",
       "  'name': 'get_data.py',\n",
       "  'size': 4113,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List files in the associated dataset\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping data.zip, file exists\n"
     ]
    }
   ],
   "source": [
    "# Retrieve `data.zip`\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for f in files:\n",
    "    if f[\"name\"] in files_to_dl:\n",
    "        if os.path.isfile(f\"{output_directory}/{f['name']}\"):\n",
    "            print(f\"Skipping {f['name']}, file exists\")\n",
    "        else:\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "            urlretrieve(f[\"download_url\"], f\"{output_directory}/{f['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract `data.zip`\n",
    "output_zip_file = os.path.join(output_directory, \"./data.zip\")\n",
    "with zipfile.ZipFile(output_zip_file, 'r') as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of the combined CSV\n",
    "combined_path = f'{output_directory}/combined_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/NorESM2-MM_daily_rainfall_NSW.csv',\n",
       " '../data/INM-CM4-8_daily_rainfall_NSW.csv',\n",
       " '../data/AWI-ESM-1-1-LR_daily_rainfall_NSW.csv',\n",
       " '../data/ACCESS-ESM1-5_daily_rainfall_NSW.csv',\n",
       " '../data/MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv',\n",
       " '../data/NorESM2-LM_daily_rainfall_NSW.csv',\n",
       " '../data/CMCC-CM2-HR4_daily_rainfall_NSW.csv',\n",
       " '../data/TaiESM1_daily_rainfall_NSW.csv',\n",
       " '../data/FGOALS-g3_daily_rainfall_NSW.csv',\n",
       " '../data/CMCC-ESM2_daily_rainfall_NSW.csv',\n",
       " '../data/CMCC-CM2-SR5_daily_rainfall_NSW.csv',\n",
       " '../data/CanESM5_daily_rainfall_NSW.csv',\n",
       " '../data/MPI-ESM1-2-LR_daily_rainfall_NSW.csv',\n",
       " '../data/BCC-CSM2-MR_daily_rainfall_NSW.csv',\n",
       " '../data/SAM0-UNICON_daily_rainfall_NSW.csv',\n",
       " '../data/ACCESS-CM2_daily_rainfall_NSW.csv',\n",
       " '../data/INM-CM5-0_daily_rainfall_NSW.csv',\n",
       " '../data/BCC-ESM1_daily_rainfall_NSW.csv',\n",
       " '../data/MPI-ESM1-2-HR_daily_rainfall_NSW.csv',\n",
       " '../data/NESM3_daily_rainfall_NSW.csv',\n",
       " '../data/EC-Earth3-Veg-LR_daily_rainfall_NSW.csv',\n",
       " '../data/GFDL-CM4_daily_rainfall_NSW.csv',\n",
       " '../data/GFDL-ESM4_daily_rainfall_NSW.csv',\n",
       " '../data/KIOST-ESM_daily_rainfall_NSW.csv',\n",
       " '../data/MIROC6_daily_rainfall_NSW.csv',\n",
       " '../data/MRI-ESM2-0_daily_rainfall_NSW.csv',\n",
       " '../data/FGOALS-f3-L_daily_rainfall_NSW.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather a list of files of CSV to merge\n",
    "files = glob.glob(f'{output_directory}/*.csv')\n",
    "files.remove(f'{output_directory}/observed_daily_rainfall_SYD.csv')\n",
    "files.remove(combined_path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_merge = [\"time\", \"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 49s, sys: 39.9 s, total: 8min 28s\n",
      "Wall time: 8min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# A Pythonic way (but not the most memory-efficient way) for merging the data\n",
    "df = pd.concat((pd.read_csv(f, index_col=0, usecols=columns_to_merge)\n",
    "                .assign(model=re.findall(r\"/([^_/]*)\", f)[-1])\n",
    "                for f in files)\n",
    "              )\n",
    "df.to_csv(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6G\t../data/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62467843, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>NorESM2-MM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>NorESM2-MM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>NorESM2-MM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>NorESM2-MM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>-35.811518</td>\n",
       "      <td>-34.86911</td>\n",
       "      <td>140.625</td>\n",
       "      <td>141.875</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>NorESM2-MM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       lat_min   lat_max  lon_min  lon_max  rain (mm/day)  \\\n",
       "time                                                                        \n",
       "1889-01-01 12:00:00 -35.811518 -34.86911  140.625  141.875       0.513492   \n",
       "1889-01-02 12:00:00 -35.811518 -34.86911  140.625  141.875       0.000923   \n",
       "1889-01-03 12:00:00 -35.811518 -34.86911  140.625  141.875       0.000009   \n",
       "1889-01-04 12:00:00 -35.811518 -34.86911  140.625  141.875       0.000025   \n",
       "1889-01-05 12:00:00 -35.811518 -34.86911  140.625  141.875       0.000013   \n",
       "\n",
       "                          model  \n",
       "time                             \n",
       "1889-01-01 12:00:00  NorESM2-MM  \n",
       "1889-01-02 12:00:00  NorESM2-MM  \n",
       "1889-01-03 12:00:00  NorESM2-MM  \n",
       "1889-01-04 12:00:00  NorESM2-MM  \n",
       "1889-01-05 12:00:00  NorESM2-MM  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken  |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-----------:|\n",
    "> | Chen, Ziyi   | macOS 13.2.1     | 32GB | M1 (10 core)      | YES    |  3min 30s   |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    |  5min 56s   |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    |  3min 17s   |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    |  6min 34s   |\n",
    "> \n",
    "> Table 1: Time taken to combine the CSV files\n",
    "\n",
    "#### Observations from Combining Data\n",
    "\n",
    "Table 1 above summarizes the results of the time trials for combining the data on our different computers.  We can see that the **Operating System** and/or **Processor** affects the amount of time taken to combine the files.  It is clear that the MacOS operating system and the M1/M2 processors performed the best as they took the least amount of time to combine the files; however, it is interesting to note that the M2 processor did not perform significantly faster than the M1 and it appears that the difference in RAM between these two computers (32GB with M1 vs. 16GB with M2) did not have an impact either.  Although, perhaps the advances in the M2 are masking the difference that would result from an increase in RAM, but that is not possible to determine given the testing completed above.\n",
    "\n",
    "Furthermore, we can see that the Windows operating system with an Intel i7 processor performed quite a bit slower than the MacOS operating system with the M1/M2 processors since it took almost double the time to combine the files.  Finally, the computer with a Linux operating system and an AMD processor performed was the slowest of our four computers at completing this task as it took the longest to combine the files (about 67% longer than the Windows computer and about three times as long as the MacOS computers).\n",
    "\n",
    "It should be noted that due to the specifications of the four computers above and the testing format, it is not possible to directly determine whether the differences in run times are due to the differences in **Operating System** or **Processors**; however, due to the similar times between the M1 and M2 processors it is possible that the operating systems account for the biggest difference, but differences in RAM between these two computers makes it difficult to confidently determine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "This is the baseline time needed to load the CSV file as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 23.4 s, total: 1min 41s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(f\"{output_directory}/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   time           object \n",
      " 1   lat_min        float64\n",
      " 2   lat_max        float64\n",
      " 3   lon_min        float64\n",
      " 4   lon_max        float64\n",
      " 5   rain (mm/day)  float64\n",
      " 6   model          object \n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 3.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | macOS 13.2.1     | 32GB | M1 (10 core)      | YES    | 34s           | 3.3+ GB      |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 1min 16s      | 3.3+ GB      |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 31.6s         | 3.3+ GB      |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 1min 28s      | 3.3+ GB      |\n",
    "> \n",
    "> Table 2: Time taken to read the combined CSV (baseline)\n",
    "\n",
    "#### Observations from Baseline\n",
    "\n",
    "Table 2 above summarizes the results of the time trials for loading the file on our different computers.  We can see that the **Operating System** and/or **Processor** again affects the amount of time taken to load the files.  The computers with the MacOS operating system and the M1/M2 processors were again the fastest as they took about half as long as the computer with the Windows operating system and an Intel processor and about one fifth of the time that it took the computer with the Linux operating system and AMD processor to load the combined CSV file to memory.\n",
    "\n",
    "We can also see here that the M2 processor did not perform significantly faster than the M1, but this could again be due to the differences in RAM between these two computers as referenced previously.  \n",
    "\n",
    "These results will be used as the baseline for comparisons of run times utilizing different approaches to reduce memory usage while performing EDA below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Change the `dtype` of the data\n",
    "\n",
    "We notice that by default it uses `float64` if we do not specify it. First, we try to see if switching to `float32` would make a smaller memory footprint, as well as a faster time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 16 s, total: 1min 33s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_float32 = pd.read_csv(f\"{output_directory}/combined_data.csv\", dtype={\n",
    "    'lat_min': 'float32',\n",
    "    'lat_max': 'float32',\n",
    "    'lon_min': 'float32',\n",
    "    'lon_max': 'float32',\n",
    "    'rain (mm/day)': 'float32'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   time           object \n",
      " 1   lat_min        float32\n",
      " 2   lat_max        float32\n",
      " 3   lon_min        float32\n",
      " 4   lon_max        float32\n",
      " 5   rain (mm/day)  float32\n",
      " 6   model          object \n",
      "dtypes: float32(5), object(2)\n",
      "memory usage: 2.1+ GB\n"
     ]
    }
   ],
   "source": [
    "df_float32.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear it from memory after the experiment\n",
    "df_float32 = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | macOS 13.2.1     | 32GB | M1 (10 core)      | YES    | 31.7s         | 2.1+ GB      |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 1min 11s      | 2.1+ GB      |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 30.5s         | 2.1+ GB      |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 1min 22s      | 2.1+ GB      |\n",
    "> \n",
    "> Table 3: Time taken to read the combined CSV (approach 1: use `float32` instead of `float64`)\n",
    "\n",
    "#### Observations from Approach 1\n",
    "\n",
    "Table 3 above summarizes the results of the time trials for loading the file on our different computers after switching the data type from `float64` to `float32` for numeric columns.  The same trend in differences of run times for this task between the computers due to differing **Operating Systems** and **Processors** is still observed so we will focus on the differences for each computer compared to it's baseline run time for loading the file.\n",
    "\n",
    "We can see that the memory usage has successfully been reduced from 3.3+ GB to 2.1+ GB across all four computers; however, there was not a significant reduction in run time for any of the four computers compared to the baseline results.  We see that the reductions in time do scale accordingly with the baseline run times as the slowest computer at this task (Linux OS with AMD processor) at baseline also had the largest reduction in run time with about a 10% reduction, while the fastest computer at this task (MacOS with M2) at baseline had the lowest reduction in run time with about a 3.5% reduction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Load only the column(s) we want\n",
    "\n",
    "The dataset contains a number of columns that we may not need to use in one go. In this approach, we try to just load one column (`rain (mm/day)` here) from the combined CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.7 s, sys: 5.43 s, total: 36.2 s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_only_rain = pd.read_csv(f\"{output_directory}/combined_data.csv\", usecols=[\"rain (mm/day)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62467843 entries, 0 to 62467842\n",
      "Data columns (total 1 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   rain (mm/day)  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 476.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_only_rain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear it from memory after the experiment\n",
    "df_only_rain = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> | Team Member  | Operating System | RAM  | Processor         | Is SSD | Time taken    | Memory usage |\n",
    "> |:------------:|:----------------:|:----:|:-----------------:|:------:|:-------------:|:------------:|\n",
    "> | Chen, Ziyi   | macOS 13.2.1     | 32GB | M1 (10 core)      | YES    | 16.1s         | 476.6 MB     |\n",
    "> | Guron, Mike  | Windows 11       | 16GB | Intel i7-12700H   | YES    | 37.1s         | 476.6 MB     |\n",
    "> | Raina, Roan  | macOS 13.2.1     | 16GB | M2 (8 core)       | YES    | 14.9s         | 476.6 MB     |\n",
    "> | Wong, Kelvin | Linux Mint 21    | 16GB | AMD Ryzen 5 3500U | YES    | 36.7s         | 476.6 MB     |\n",
    "> \n",
    "> Table 4: Time taken to read the combined CSV (approach 2: just load `rain (mm/day)`)\n",
    "\n",
    "#### Observations from Approach 2\n",
    "\n",
    "Table 4 above summarizes the results of the time trials for loading the file on our different computers when loading just one column of the file.  The same trend in differences of run times for this task between the computers due to differing **Operating Systems** and **Processors** is still observed so we will focus on the differences for each computer compared to it's baseline run time for loading the file.\n",
    "\n",
    "We can see that the memory usage has been even further reduced from 3.3+ GB for the whole file to just 476.6 MB using this approach, which also resulted in a significant reduction in run times as the time to load the file has approximately been cut in half across all four computers compared to the baseline.  Again, we see that the reductions in time do scale accordingly with the baseline run times as the slowest computer at this task (Linux OS with AMD processor) at baseline also had the largest reduction in run time with about a 60% reduction, while the fastest computer at this task (MacOS with M2) at baseline had the lowest reduction in run time with about a 53% reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explore the EDA in R instead of Python. We try \"exporting\" our data frame as a Parquet file for processing in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 9.08 s, total: 30.7 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.to_parquet(f\"{output_directory}/combined_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the Python dataset from memory\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why we choose Parquet?\n",
    "> \n",
    "> (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘arrow’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:utils’:\n",
      "\n",
      "    timestamp\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 7.43 s, total: 18.3 s\n",
      "Wall time: 8.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_parquet <- open_dataset(\"../data/combined_data.parquet\")\n",
    "r_df <- r_parquet |> collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 62467843        7\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "r_df |> dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "class(r_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 62,467,843\n",
      "Columns: 7\n",
      "$ time            <chr> \"1889-01-01 12:00:00\", \"1889-01-02 12:00:00\", \"1889-01…\n",
      "$ lat_min         <dbl> -35.81152, -35.81152, -35.81152, -35.81152, -35.81152,…\n",
      "$ lat_max         <dbl> -34.86911, -34.86911, -34.86911, -34.86911, -34.86911,…\n",
      "$ lon_min         <dbl> 140.625, 140.625, 140.625, 140.625, 140.625, 140.625, …\n",
      "$ lon_max         <dbl> 141.875, 141.875, 141.875, 141.875, 141.875, 141.875, …\n",
      "$ `rain (mm/day)` <dbl> 5.134920e-01, 9.230450e-04, 9.390591e-06, 2.520761e-05…\n",
      "$ model           <chr> \"NorESM2-MM\", \"NorESM2-MM\", \"NorESM2-MM\", \"NorESM2-MM\"…\n",
      "CPU times: user 16.5 s, sys: 1.33 s, total: 17.9 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 6 × 7\n",
      "  time                lat_min lat_max lon_min lon_max `rain (mm/day)` model     \n",
      "  <chr>                 <dbl>   <dbl>   <dbl>   <dbl>           <dbl> <chr>     \n",
      "1 1889-01-01 12:00:00   -35.8   -34.9    141.    142.      0.513      NorESM2-MM\n",
      "2 1889-01-02 12:00:00   -35.8   -34.9    141.    142.      0.000923   NorESM2-MM\n",
      "3 1889-01-03 12:00:00   -35.8   -34.9    141.    142.      0.00000939 NorESM2-MM\n",
      "4 1889-01-04 12:00:00   -35.8   -34.9    141.    142.      0.0000252  NorESM2-MM\n",
      "5 1889-01-05 12:00:00   -35.8   -34.9    141.    142.      0.0000133  NorESM2-MM\n",
      "6 1889-01-06 12:00:00   -35.8   -34.9    141.    142.      0.0000129  NorESM2-MM\n",
      "CPU times: user 111 ms, sys: 10.6 ms, total: 122 ms\n",
      "Wall time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 6 × 7\n",
      "  time                lat_min lat_max lon_min lon_max `rain (mm/day)` model     \n",
      "  <chr>                 <dbl>   <dbl>   <dbl>   <dbl>           <dbl> <chr>     \n",
      "1 2014-12-26 12:00:00   -29.9   -29.1    153.    154.          0.196  FGOALS-f3…\n",
      "2 2014-12-27 12:00:00   -29.9   -29.1    153.    154.          9.80   FGOALS-f3…\n",
      "3 2014-12-28 12:00:00   -29.9   -29.1    153.    154.         11.0    FGOALS-f3…\n",
      "4 2014-12-29 12:00:00   -29.9   -29.1    153.    154.          0.706  FGOALS-f3…\n",
      "5 2014-12-30 12:00:00   -29.9   -29.1    153.    154.          0.840  FGOALS-f3…\n",
      "6 2014-12-31 12:00:00   -29.9   -29.1    153.    154.          0.0508 FGOALS-f3…\n",
      "CPU times: user 73.6 ms, sys: 13.6 ms, total: 87.1 ms\n",
      "Wall time: 86.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_df |> tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsci525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
