{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4\n",
    "\n",
    "In this milestone, we are deploying our learned model to Amazon EC2. We are using Flask to host the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the API\n",
    "\n",
    "Here, we have created a simple Flask application that would return a prediction value at a `POST` endpoint.\n",
    "\n",
    "We downloaded the `model.joblib` from Milestone 3, installed the dependencies (`flask`, `scikit-learn`, `pandas`), and placed that at a project directory. We saved the code below as `main.py` and placed that at the project directory as well. Then we run:\n",
    "\n",
    "```\n",
    "flask --app main --host 0.0.0.0 --debug\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install flask scikit-learn joblib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "def compute_prediction(data):\n",
    "    df = pd.DataFrame(data).T\n",
    "    return model.predict(df).tolist()\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return \"\"\"\n",
    "    <h1>Rain Prediction Service</h1>\n",
    "    <p>\n",
    "        <strong>Usage</strong>:<br />\n",
    "        Make a JSON post request to the /predict url with 25 climate model outputs.\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>Example</strong>:<br />\n",
    "        <code>curl http://127.0.0.1:5000/predict\n",
    "            -d '{\"data\":[1,2,3,4,53,11,22,37,41,53,11,24,31,44,53,11,22,35,42,53,12,23,31,42,53]}'\n",
    "            -H \"Content-Type: application/json\"\n",
    "        </code>\n",
    "    </p>\n",
    "    \"\"\"\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def rainfall_prediction():\n",
    "    content = request.json\n",
    "    prediction = compute_prediction(content[\"data\"])\n",
    "    results = {\n",
    "        \"prediction\": prediction,\n",
    "        \"input\": content[\"data\"]\n",
    "    }\n",
    "    return jsonify(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the API\n",
    "\n",
    "We have used AWS EC2 for this lab. After making sure that the security group allows the port that we need (`5000` in our case here), we run the Flask app. Here is a screenshot showing the API in action.\n",
    "\n",
    "![Screenshot showing the browser output of the `/` endpoint](images/m4-browser.png)\n",
    "\n",
    "![Screenshot showing the curl output of the `/predict` endpoint](images/m4-curl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone Summaries\n",
    "\n",
    "This is a summary of what we have done across the milestones:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Milestone 1**:\n",
    "In milestone 1, we downloaded the data, combined the csv files, and preformed a EDA in both python and R. During this process we investigated csv join, load, and data type transformation performance between computers (CPU and RAM) while measuring RAM used. We then exported the data from python to R using the parquet file format to preform further EDA in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Milestone 2**: In Milestone 2, we made the transition from working on our local machines to the cloud using Amazon Web Services (AWS).  We created a collaborative environment by setting up an EC2 instance with JupyterHub, installed all the necessary packages needed for our server, and added all team members as users.  We then set up a S3 bucket to read and store the data from the parquet file created in Milestone 1.  Once the data was read and loaded into the bucket, it was wrangled in preparation for use in machine learning models in Milestone 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Milestone 3**: Initially, we performed some fundamental exploratory data analysis on the CSV file retrieved from S3. Subsequently, we partitioned the data into training and testing sets and proceeded to train an ensemble machine learning model using the RandomForestRegressor algorithm on the ml_data_SYD.csv file generated in Milestone 2. We evaluated the performance of the model using RMSE (root-mean-square error). To identify the optimal model, we utilized PySpark's MLlib to fine-tune some hyperparameters of a Random Forest and deployed the resulting model using Amazon EMR."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Milestone 4**:\n",
    "We used AWS EC2, a scalable virtual server, to host a flask app that returns a prediction using the model developed and trained in previous milestones via the POST endpoint `predict`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
